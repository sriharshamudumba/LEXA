# LEXA: Local Retrieval-Augmented Generation System

LEXA is a lightweight, fully local Retrieval-Augmented Generation (RAG) system that uses a small language model (TinyLlama 1.1B) to answer user queries based on relevant documents stored locally.

---

## ğŸ”§ Features

- ğŸ” Semantic search using SentenceTransformers + FAISS
- ğŸ§  Local language generation using TinyLlama (HuggingFace)
- âš¡ FastAPI backend for API exposure
- ğŸ’¬ Supports both document-grounded answers and freeform LLM prompts

---

## ğŸ—ï¸ Project Structure

LEXA/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ app.py # FastAPI app with endpoints
â”‚ â”œâ”€â”€ generation.py # TinyLlama-based response generation
â”‚ â”œâ”€â”€ retrieval.py # Semantic document retrieval logic
â”œâ”€â”€ data/
â”‚ â””â”€â”€ documents/ # Local .txt files used for context
â”œâ”€â”€ requirements.txt

---

## ğŸš€ Quickstart

### 1. Clone the repository
```bash
git clone https://github.com/your-username/LEXA.git
cd LEXA

2. Set up the virtual environment
bash
Copy
Edit
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt



3. Add your documents
Place .txt files in: data/documents/

4. Start the API server
uvicorn backend.app:app --reload --port 8000


5. Test the endpoints
Go to: http://127.0.0.1:8000/docs

